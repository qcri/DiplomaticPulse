import scrapy
from scrapy.loader import ItemLoader
from scrapy.loader.processors import TakeFirst, MapCompose
from diplomaticpulse.items import StatementItem
from datetime import datetime
import  urllib.parse, random, w3lib.html
from diplomaticpulse.db.getUrlConfigs import DpElasticsearch
from diplomaticpulse.utilities import date_parser, errback_http, inspect_html
from scrapy.utils.project import get_project_settings
from scrapy.exceptions import CloseSpider

__author__ = "Abdelkader Lattab"
__email__ = 'alattab@hbku.edu.qa'
__copyright__ = "???"
__license__ = "???"
__version__ = "1.0.0"

class StatementSpider(scrapy.spiders.Spider):
    """Crawling  spider  for static html scraping
        Parameters
            Spider instance

        Returns
        Item class instance
    """
    name = "html"

    def __init__(self, url, *args, **kwargs):
        self.logger.info('HTML Spider version 0.1.135')
        self.statements_seen = set()
        self.settings = get_project_settings()
        self.url = url #urllib.parse.unquote(url).replace('"', "")
        self.xpaths = {}
        self.urls_seen=[]
        self.content = 'html'
        self.posted_date_format=None
        self.test_mode=None

    def start_requests(self):
        """Method which starts the requests by visiting all URLs specified in start_urls

            Args: self

            Returns:
            make request for the main URL (start url)

            Raises:
            CloseSpider
        """
        self.start_urls = [self.url]
        self.logger.info('starting  start_urls  %s ', self.start_urls)
        # pick up a random user agent
        self.headers = {'User-Agent': random.choice(self.settings['USER_AGENT_LIST'])}

        #read xpaths from Elasticsearch
        #Note that , it uses difefrent elasticsearch link, when running outside Scrapyd server
        self.logger.info('---->connecting to   %s ', self.settings['ELASTIC_HOST'])
      #  es_server = self.settings['ELASTIC_HOST']
        es_server = self.settings['ELASTIC_HOST']
        self.logger.info('---- es_server %s' % es_server)
        self.logger.info('---- url %s' % self.url)
        self.logger.info('---- index: %s' % self.settings['ELASTIC_INDEX_SITECONFIG'])
        es = DpElasticsearch(es_server)
        # res = es.search_by_url(self.url, self.settings['ELASTIC_INDEX_SITECONFIG'])
        res = es.search_by_url('press', self.settings['ELASTIC_INDEX_SITECONFIG'])
        if not res:
            self.logger.info('---- res %s', res)
            raise CloseSpider('No Xpaths data found for the url ')
        self.xpaths = res[0]['_source']
        slices = self.xpaths.get('posted_date').split('#')
        self.xpaths['posted_date'] = slices[0]
        if len(slices)> 1:
            self.posted_date_format = slices[1]

        if self.settings['URLS_ALREADY_SEEN']:
            self.urls_seen = es.search_by_country_and_type(self.xpaths['name'], self.content,self.settings['ELASTIC_INDEX'])

        self.logger.info('urls seen already  from %s ', (self.urls_seen))
        #start sending request for each overview website url
        for url in self.start_urls:
             yield scrapy.Request(url,dont_filter=True,headers=self.headers, callback=self.parse)

    def parse(self, response):
        """parse response content url

        Args: response

        Returns:
          send url request, and follow more links
        """
        self.logger.info('a response from %s just arrived!', response.url)
        blocks=response.xpath(self.xpaths['global']).getall()
        self.logger.info('scraped html blocks  %s  from starting page', str(len(blocks)))
        not_seen_before = inspect_html.urls_to_request(response,self.urls_seen, blocks, self.xpaths)
        self.logger.info(' not_seen_before  %s  from starting page',len(not_seen_before))
        for key, value in  not_seen_before.items():
            url =  response.urljoin(key)
            self.logger.info('--->send request for url %s', url)
            request = scrapy.Request(url, callback=self.parseitem, errback=errback_http.errback_httpbin, headers=self.headers,
                                     cb_kwargs=dict(title=value['title'], posted_date=value['posted_date']))
            yield request

        #list of links scraped from the meach mail URL
        try:
            next_page_url = response.xpath(self.xpaths['next_page']).get()
            if next_page_url:
                url = response.urljoin(next_page_url)
                yield response.follow(url, callback=self.parse)
        except:
            pass

    def parseitem(self,response, title, posted_date):
        """parse response content url

        Args: response,html_block

        Returns:
          send Item instance to middleware
        """
        self.logger.info(' --->start parsing  item from %s !', response.url)
        if not title:
            title = response.xpath(self.xpaths['title']).get()
        if not posted_date:
            posted_date = response.xpath(self.xpaths['posted_date']).get()
        Item_loader = ItemLoader(item=StatementItem(), response=response)
        Item_loader.default_input_processor = MapCompose(str())
        Item_loader.default_output_processor = TakeFirst()
        url = urllib.parse.unquote(response.url)
        Item_loader.add_value('url', url)
        Item_loader.add_value('indexed_date', (datetime.now()).strftime("%Y-%m-%d %H:%M:%S"))
        Item_loader.add_value('content_type', self.xpaths['content_type'])
        Item_loader.add_value('language', '')
        Item_loader.add_value('country', self.xpaths['name'])
        text = ' '.join(response.xpath(self.xpaths['statement']).getall())
        text = inspect_html.clean_text(text.strip(), self.xpaths['tag'], response.url)
        self.statements_seen.add(text)
        Item_loader.add_value('statement', text)
        if not posted_date or 'nil' in self.xpaths['posted_date']:
            try:
                posted_date = date_parser.date_finder(w3lib.html.remove_tags(title).replace('\r\n', ''))
            except:
                pass

        if not posted_date:
            #try date from end last 100 characters
            try:
                my_text = w3lib.html.remove_tags(text).replace('\r\n', '')
                my_text = my_text[-100:]
                posted_date = date_parser.date_finder(my_text)
            except:
                pass
        if not posted_date:
            #try date from 100 characters text
            try:
                my_text = w3lib.html.remove_tags(text).replace('\r\n', '')
                my_text = my_text[:100]
                posted_date = date_parser.date_finder(my_text)
            except:
                pass

        if not posted_date:
            #try date from title
            posted_date = date_parser.date_finder(title)

        if not posted_date:
            posted_date = (datetime.now()).strftime("%Y-%m-%d")

        posted_date = posted_date.replace('|', '')
        if self.posted_date_format:
            posted_date = posted_date + "#" + self.posted_date_format
        self.logger.info('url: %s   date posted: %s', response.url, posted_date)
        if not title or len(title) < 20: #minimu length allowed is 20 characters
            # grep first 200 characters
            title = text[:200]
        Item_loader.add_value('title', title.strip())
        Item_loader.add_value('posted_date', str(posted_date))
        #self.logger.info(' -----> %s  %s  %s', response.url, title, posted_date)
        yield Item_loader.load_item()




